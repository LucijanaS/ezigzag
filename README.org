#+AUTHOR: phdenzel
#+TITLE: ezigzag
#+DATE: 2024-11-17 Sun
#+OPTIONS: author:nil title:t date:nil timestamp:nil toc:nil num:nil \n:nil


* Requirements

- Python ~>= 3.9~

For a list of python packages, inspect the ~pyproject.toml~ file, or
run
#+begin_src bash
python -c "import toml; print('\n'.join(pkg for pkg in toml.load('pyproject.toml')['project']['dependencies']))"
#+end_src


* Install

It is recommended to install and run the ~ezigzag~ project in a
virtual environment.


** Virtualenv

The easiest method to install a virtual environment is
#+begin_src bash
  virtualenv venv
  source venv/bin/activate
  pip install --upgrade pip
#+end_src


** Conda environment

If ~conda~ is already installed, you can alternatively use
#+begin_src bash
  conda create -n ezigzag python=3.12 -y
  conda activate ezigzag
#+end_src


** Install the project

Install the project in editable mode with
#+begin_src bash
pip install -e .
#+end_src


** Datasets

Test datasets can be downloaded (via ~curl~) from the FTPS server
#+begin_src bash
  cd data && FTPPASS="..." ./download_simpsons.sh
#+end_src


*** IllustrisTNG projections

A subset of the IllustrisTNG galaxy map projection dataset (around 6GB) can be downloaded (via ~curl~) from the FTPS server
#+begin_src bash
  cd data && FTPPASS="..." ./download_tng2D_subset.sh
#+end_src


For loading samples from the dataset, use
#+begin_src python
  from ezigzag import parse_args
  from ezigzag.data import HDF5Dataset, TrainTestDataLoader

  ds = HDF5Dataset("./data", meta_groups="**/metadata/*", collate=True)
  dl = TrainTestDataLoader(ds, ds, batch_size=1, shuffle=False)
  for i, batch in dl.tqdm():

        maps = batch[0]  # batched mass maps of shape (batch_size, 512, 512)
        maps = maps.detach().cpu().numpy()  # to convert to numpy arrays
        info = batch[1]  # dictionary with metadata like extent, units, etc.
        if i == 0:
              break
        ...
#+end_src


* How to train the model

In ~ezigzag/train.py~ a Variational Autoencoder is trained on the
~simpsons~ dataset. Edit the file as you wish (especially the VAE
keyword arguments) and run it on a device as powerful as possible
(~mps~ for macOS M1-4 GPUs, ~cuda~ for Nvidia GPUs, or ~cpu~ for a
CPU) with e.g.

#+begin_src bash
  python ezigzag/train.py --device cpu
#+end_src


** Tips

- Higher batch sizes e.g. ~--batch-size 8~, result in faster training
- Increasing the number of down and up blocks in the VAE (manually) likely
  results in better image recostructions at the cost of compute, e.g.
  ~down_block_types=3*("EncoderDownBlock",)~,
  ~up_block_types=3*("EncoderUpBlock",)~, and
  ~block_out_channel_mults=3*(2,)~.
- In the initial epochs, ~train_ELBO~ (the KL divergence) will be high,
  and should go below the ~train_MSE~ (the L2 loss) term. This
  indicates that the regularization of the latent space works, i.e. the
  likelihoods of neighboring points are close to Gaussian.
- ~PSNR~ is a evaluation metric (higher is better) in logarithmic
  scale and measures the reconstruction quality above the noise level.
  This value should increase with time.


* After training

Load a trained model (here with ID ~febcd77b~ as an example) with the
~load_ckpt~ function, e.g.

#+begin_src python
  model = VAE(
      dimensions=2,
      in_channels=3,
      n_channels=32,
      latent_dim=4,
      res_act_fn=kwargs["activation"],
      res_dropout=kwargs["dropout"],
      block_out_channel_mults=(2, 2),
      down_block_types=("EncoderDownBlock", "EncoderDownBlock"),
      mid_block_type="EncoderMidBlock",
      up_block_types=("EncoderUpBlock", "EncoderUpBlock"),
  )
  model, _ = load_ckpt(
      "checkpoints/febcd77b_epoch_latest.pth",
      model
  )
  reconstructed_image = model.decode(torch.randn((1, 8, 128, 128)))\
     .detach()\
     .cpu()\
     .squeeze()\
     .permute(1, 2, 0)\
     .numpy()
#+end_src

